{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fmore\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "assert nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_src_dataset = Path(\"./data/src/spotify_tracks.csv\")\n",
    "\n",
    "df = pd.read_csv(path_src_dataset, nrows=10000) # Dataframe used to test functions, we can only take few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(ABC, BaseEstimator, TransformerMixin):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "# EXAMPLE OF TRANSFORMER FOR CLEANING / PROCESSING\n",
    "class NewTransformer(Transformer):\n",
    "    def __init__(self):\n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        #TODO\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        #TODO\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Create a *Pipeline* which is a series of *Tranformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNaRate(Transformer):\n",
    "    def __init__(self, rate: float):\n",
    "        self.rate = rate\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        perc_na = X.isna().sum()/X.shape[0]\n",
    "        self.cols_to_drop: pd.Series = perc_na[perc_na > self.rate].index\n",
    "\n",
    "        print(f\">> (Info) Droped columns : {self.cols_to_drop.to_list()}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X = X.drop(columns = self.cols_to_drop)\n",
    "\n",
    "        return X\n",
    "    \n",
    "### TEST ###\n",
    "\n",
    "transfo = DropNaRate(0.02)\n",
    "transfo.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanTextData(Transformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            columns: list[str],\n",
    "            extended_stopwords_list: list=[],\n",
    "            extended_ponctuation_pattern: re.Pattern[str] | str=r'',\n",
    "        ):\n",
    "        self.columns = columns\n",
    "        self.extended_stopwords_list = extended_stopwords_list\n",
    "        self.extended_ponctuation_pattern = extended_ponctuation_pattern\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        stopwords = nltk.corpus.stopwords.words(fileids=('english', 'spanish', 'french'))\n",
    "        stopwords = [word.replace(\"\\n\",\"\") for word in stopwords]\n",
    "        stopwords.extend(self.extended_stopwords_list)\n",
    "\n",
    "        punctuation_pattern = r'[\\[\\]()\\-:;\",/\\.\\.\\.‘\\'’?!“&]' + self.extended_ponctuation_pattern\n",
    "\n",
    "        for col in self.columns:\n",
    "\n",
    "            clean_cells = []\n",
    "            for cell in X[col].to_list():\n",
    "                cell: str\n",
    "\n",
    "                # remove punctuation\n",
    "                cell = cell.lower()\n",
    "                cell = re.sub(punctuation_pattern, \"\", cell).replace(\"  \",\" \")\n",
    "\n",
    "                # remove stopwords\n",
    "                clean_text = [text for text in cell.split(\" \") if text not in stopwords]\n",
    "                clean_text = \" \".join(clean_text)\n",
    "                clean_cells.append(clean_text)\n",
    "            \n",
    "            X[col] = clean_cells\n",
    "\n",
    "        print(f\">> (Info) Punctuation and stopwords removed from columns {self.columns}\")\n",
    "\n",
    "        return X\n",
    "    \n",
    "### TEST ###\n",
    "\n",
    "transfo = CleanTextData([\"track_name\"])\n",
    "transfo.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_src_dataset = Path(\"./data/src/spotify_tracks.csv\")\n",
    "out_folder_dataset = Path(\"./data/cleaned\")\n",
    "out_folder_config = Path(\"./data/cleaned/pipelines\")\n",
    "\n",
    "df = pd.read_csv(path_src_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> (Info) Droped columns : []\n",
      ">> (Info) Punctuation and stopwords removed from columns ['track_name']\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    (\"DropNaRate\", DropNaRate(0.7)),\n",
    "    (\"CleanTextData\", CleanTextData([\"track_name\"])),\n",
    "    # ... Add others transformations\n",
    "])\n",
    "\n",
    "cleaned_df = pipeline.fit_transform(df)\n",
    "# cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> (Info) Droped columns : []\n",
      ">> (Info) Punctuation and stopwords removed from columns ['track_name']\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = \"TODELETE\"\n",
    "\n",
    "with open(out_folder_config / Path(pipeline_name + \".pkl\"), 'rb') as file:\n",
    "    pipeline: Pipeline = pickle.load(file)\n",
    "\n",
    "\n",
    "cleaned_df = pipeline.fit_transform(df)\n",
    "# cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Dataset + Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_name = \"TODELETE\"\n",
    "\n",
    "cleaned_df.to_csv(out_folder_dataset / Path(cleaned_df_name + \".csv\"))\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(out_folder_config / Path(cleaned_df_name + \".pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
