{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.svm import LinearSVC\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "assert nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_src_dataset = Path(\"./data/src/X_train_Hi5.csv\")\n",
    "\n",
    "df = pd.read_csv(path_src_dataset, nrows=10000) # Dataframe used to test functions, we can only take few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(ABC, BaseEstimator, TransformerMixin):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "# EXAMPLE OF TRANSFORMER FOR CLEANING / PROCESSING\n",
    "class NewTransformer(Transformer):\n",
    "    def __init__(self):\n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        #TODO\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        #TODO\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumns(Transformer):\n",
    "    def __init__(self, cols_to_drop=[]):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        print(f\">> (Info) Dropped columns : {self.cols_to_drop}\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.drop(columns=self.cols_to_drop)\n",
    "\n",
    "        return X\n",
    "\n",
    "# cols_to_drop = ['piezo_station_department_name',\n",
    "#                 'piezo_station_update_date', 'piezo_station_commune_code_insee', 'piezo_station_pe_label', 'piezo_station_bdlisa_codes', 'piezo_station_bss_code', 'piezo_station_bss_id', 'piezo_bss_code', 'piezo_measurement_date', 'piezo_producer_name', 'piezo_measure_nature_code', 'meteo_name', ]\n",
    "drop_col = DropColumns(cols_to_drop=['piezo_station_update_date'])\n",
    "df = drop_col.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting dates transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(Transformer):\n",
    "    def __init__(self):\n",
    "        self.date_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.date_cols = [col for col in X.columns if 'date' in col]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.date_cols :\n",
    "            if col =='meteo_date':\n",
    "                X[col] = pd.to_datetime(X[col], errors='coerce').apply(lambda x: np.cos(x * 2 * np.pi / 365.25))\n",
    "            else:\n",
    "                X.drop(col, axis=1, inplace=True)\n",
    "            X.rename(columns={'meteo_date': 'date'}, inplace=True)          \n",
    "        return X\n",
    "    \n",
    "date_transformer = DateTransformer()\n",
    "date_transformer.fit(df)\n",
    "df_dates = date_transformer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a *Pipeline* which is a series of *Tranformers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumCols(Transformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            columns: list[str],\n",
    "            weights: list[float]=[],\n",
    "            new_col_name: str=None,\n",
    "            remove_cols_in: bool=False,\n",
    "        ):\n",
    "        \n",
    "        assert len(columns) > 1, \">> (ERROR - SumCols) 2 columns are required\"\n",
    "        self.columns = columns\n",
    "        \n",
    "        assert len(weights) == 0 or len(weights) == len(self.columns), \">> (ERROR - SumCols) columns and weights must have same dimensions.\"\n",
    "        self.weights = weights if len(weights) == 0 else [1]*len(columns)\n",
    "\n",
    "        self.new_col_name = new_col_name if new_col_name is not None else \"_+_\".join(self.columns)\n",
    "        self.remove_cols_in = remove_cols_in\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X[self.new_col_name] = np.dot(X[self.columns], self.weights)\n",
    "\n",
    "        print(f\">> (INFO - SumCols) columns {self.columns} has been sumed in a new column : {self.new_col_name}\")\n",
    "\n",
    "\n",
    "        if self.remove_cols_in:\n",
    "            X = X.drop(columns=self.columns)\n",
    "\n",
    "        return X\n",
    "    \n",
    "### TEST ###\n",
    "\n",
    "transfo = SumCols([\"duration_ms\", \"popularity\"], [0, 2], remove_cols_in=True)\n",
    "df_test = transfo.fit_transform(df)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialOneHotEncoder(Transformer):\n",
    "    \"\"\"partial because only some columns can be selected for encoding.\"\"\"    \n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            columns: list[str],\n",
    "            *,\n",
    "            categories=\"auto\",\n",
    "            drop='if_binary',\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=None,\n",
    "            max_categories=None,\n",
    "        ):\n",
    "        self.columns = columns\n",
    "\n",
    "        self.encoder = OneHotEncoder(\n",
    "            categories=categories,\n",
    "            drop=drop,\n",
    "            sparse_output=False,\n",
    "            handle_unknown=handle_unknown,\n",
    "            min_frequency=min_frequency,\n",
    "            max_categories=max_categories,\n",
    "        )\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.encoder = self.encoder.fit(X[self.columns])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X_one_hot_encoded= self.encoder.transform(X[self.columns])\n",
    "\n",
    "        X_one_hot_df = pd.DataFrame(X_one_hot_encoded, columns=self.encoder.get_feature_names_out())\n",
    "\n",
    "        X = pd.concat([df.drop(self.columns, axis=1), X_one_hot_df], axis=1)\n",
    "\n",
    "        print(f\">> (INFO - PartialOneHotEncoder) {self.encoder.feature_names_in_} features one hot encoded as : {self.encoder.get_feature_names_out()}\")\n",
    "\n",
    "        return X\n",
    "    \n",
    "### TEST ###\n",
    "\n",
    "transfo = PartialOneHotEncoder(columns=[\"explicit\"])\n",
    "df_test = transfo.fit_transform(df)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialStandardScaler(Transformer):\n",
    "    \"\"\"partial because only some columns can be selected for standardiation.\"\"\"    \n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            columns: list[str],\n",
    "            *,\n",
    "            copy: bool = True,\n",
    "            with_mean: bool = True,\n",
    "            with_std: bool = True\n",
    "        ):\n",
    "        self.columns = columns\n",
    "        self.standardizer = StandardScaler(\n",
    "            copy=copy,\n",
    "            with_mean=with_mean,\n",
    "            with_std=with_std,\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.standardizer.fit(X[self.columns])\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X_standardized_np = self.standardizer.transform(X[self.columns])\n",
    "\n",
    "        X_standardized = pd.DataFrame(X_standardized_np, columns=self.standardizer.get_feature_names_out())\n",
    "\n",
    "        X = pd.concat([df.drop(self.columns, axis=1), X_standardized], axis=1)\n",
    "\n",
    "        print(f\">> (INFO - PartialStandardScaler) columns {self.columns} have bean standardized\")\n",
    "\n",
    "\n",
    "        return X\n",
    "    \n",
    "### TEST ###\n",
    "\n",
    "transfo = PartialStandardScaler(columns=[\"energy\"])\n",
    "df_test = transfo.fit_transform(df)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropCols(Transformer):\n",
    "    def __init__(self, columns: list[str]):\n",
    "        self.columns = columns\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X = X.drop(columns=self.columns)\n",
    "\n",
    "        print(f\">> (INFO - DropCols) columns {self.columns} is/are droped.\")\n",
    "\n",
    "        return X\n",
    "    \n",
    "transfo = DropCols(columns=[\"explicit\"])\n",
    "df_test = transfo.fit_transform(df)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_src_dataset = Path(\"./data/cleaned/TODELETE.csv\")\n",
    "out_folder_dataset = Path(\"./data/processed\")\n",
    "out_folder_config = Path(\"./data/processed/pipelines\")\n",
    "\n",
    "df = pd.read_csv(path_src_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Continuous / Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=[\"object\"]).columns.to_list()\n",
    "num_categorical_features = [\"key\",\"mode\",\"time_signature\"] # Add numerical data but with a categorical meaning (ex: color of car  => red=0, blue=1, green=2)\n",
    "categorical_features.extend(num_categorical_features)\n",
    "\n",
    "numerical_features = df.drop(columns=categorical_features).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('SumCols', SumCols(columns=[\"duration_ms\", \"popularity\"], weights=[0, 2], remove_cols_in=True)),\n",
    "    ('DropCols', DropCols([\"key\"])),\n",
    "    ('OneHotEncoder', PartialOneHotEncoder(columns=[\"explicit\"])),\n",
    "    ('PartialStandardScaler', PartialStandardScaler(columns=[\"energy\"])),\n",
    "    # ... Add other Transformers\n",
    "    ('FeatureSelection', SelectFromModel(LinearSVC(penalty=\"l1\"), max_features=30)) # Feature selection\n",
    "])\n",
    "\n",
    "df_processed = pipeline.fit_transform(df)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"TODELETE\"\n",
    "\n",
    "with open(out_folder_config / Path(pipeline_name + \".pkl\"), 'rb') as file:\n",
    "    pipeline: Pipeline = pickle.load(file)\n",
    "\n",
    "\n",
    "df_processed = pipeline.fit_transform(df)\n",
    "# df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Processed Dataset + Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_name = \"TODELETE\"\n",
    "\n",
    "df_processed.to_csv(out_folder_dataset / Path(df_processed_name + \".csv\"))\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(out_folder_config / Path(df_processed_name + \".pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
